{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate \n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_width = 512\n",
    "im_height = 512\n",
    "im_chan = 1\n",
    "\n",
    "path_train = '/kaggle/input/small-dataset/Train/'\n",
    "path_test = '/kaggle/input/small-dataset/Test/'\n",
    "\n",
    "train_ids = next(os.walk(path_train+\"images\"))[2]\n",
    "test_ids = next(os.walk(path_test+\"images\"))[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((len(train_ids), im_height, im_width, im_chan), dtype=np.uint8)\n",
    "Y_train = np.zeros((len(train_ids), im_height, im_width, 1), dtype=bool)\n",
    "\n",
    "print('Getting and resizing train images and masks ... ')\n",
    "\n",
    "sys.stdout.flush()\n",
    "for n, id_ in tqdm_notebook(enumerate(train_ids), total=len(train_ids)):\n",
    "    path = path_train\n",
    "    img = load_img(path + '/images/' + id_)\n",
    "    \n",
    "    x = img_to_array(img)[:,:,1]\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    X_train[n] = x\n",
    "    \n",
    "    \n",
    "    mask = img_to_array(load_img(path + '/masks/' + id_))[:,:,1]\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    Y_train[n] = mask\n",
    "    \n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = random.randint(0, len(train_ids))\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(np.dstack((X_train[ix],X_train[ix],X_train[ix])))\n",
    "plt.title(\"Image\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "tmp = np.squeeze(Y_train[ix]).astype(np.float32)\n",
    "plt.imshow(np.dstack((tmp,tmp,tmp)))\n",
    "plt.title(\"Ground truth\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable()\n",
    "class MeanIoUMetric(metrics.Metric):\n",
    "    \n",
    "    def __init__(self, num_classes, name='mean_iou', **kwargs):\n",
    "        super(MeanIoUMetric, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes  \n",
    "        self.iou_metric = metrics.MeanIoU(num_classes=num_classes) \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_ = tf.cast(y_pred > 0.5, tf.int32)\n",
    "        self.iou_metric.update_state(y_true, y_pred_)\n",
    "\n",
    "    def result(self):\n",
    "        return self.iou_metric.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.iou_metric.reset_state()\n",
    "        \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)  \n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_classes\": self.num_classes})  \n",
    "        return config\n",
    "    \n",
    "    \n",
    "mean_iou_metric = MeanIoUMetric(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2DTranspose\n",
    "from tensorflow.keras.layers import InputSpec \n",
    "import tensorflow.keras.backend as K \n",
    "\n",
    "\n",
    "@register_keras_serializable()\n",
    "class Conv2DTranspose(Conv2D):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=(1, 1),\n",
    "        padding='valid',\n",
    "        output_shape=None,\n",
    "        data_format=None,\n",
    "        activation=None,\n",
    "        use_bias=True,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        kernel_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        activity_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(Conv2DTranspose, self).__init__(\n",
    "            filters,\n",
    "            kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            data_format=data_format,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.input_spec = InputSpec(ndim=4)\n",
    "        if output_shape is not None:\n",
    "            try:\n",
    "                self._output_shape = tuple(output_shape)\n",
    "            except TypeError:\n",
    "                raise ValueError('`output_shape` argument must be a ' +\n",
    "                                 'tuple. Received: ' + str(output_shape))\n",
    "            if len(self._output_shape) != 3:\n",
    "                raise ValueError('`output_shape` argument should have ' +\n",
    "                                 'rank ' + str(3) + '; Received:', str(output_shape))\n",
    "        else:\n",
    "            self._output_shape = output_shape\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if len(input_shape) != 4:\n",
    "            raise ValueError(\n",
    "                'Inputs should have rank ' + str(4) +\n",
    "                '; Received input shape:', str(input_shape)\n",
    "            )\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError(\n",
    "                'The channel dimension of the inputs '\n",
    "                'should be defined. Found `None`.'\n",
    "            )\n",
    "        input_dim = input_shape[channel_axis]\n",
    "        kernel_shape = self.kernel_size + (self.filters, input_dim)\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=kernel_shape,\n",
    "            initializer=self.kernel_initializer,\n",
    "            name='kernel',\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint\n",
    "        )\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\n",
    "                shape=(self.filters, ),\n",
    "                initializer=self.bias_initializer,\n",
    "                name='bias',\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint\n",
    "            )\n",
    "        else:\n",
    "            self.bias = None\n",
    "       \n",
    "        self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = K.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        if self.data_format == 'channels_first':\n",
    "            h_axis, w_axis = 2, 3\n",
    "        else:\n",
    "            h_axis, w_axis = 1, 2\n",
    "\n",
    "        height, width = input_shape[h_axis], input_shape[w_axis]\n",
    "        kernel_h, kernel_w = self.kernel_size\n",
    "        stride_h, stride_w = self.strides\n",
    "\n",
    "      \n",
    "        if self._output_shape is None:\n",
    "            out_height = deconv_length(height, stride_h, kernel_h, self.padding)\n",
    "            out_width = deconv_length(width, stride_w, kernel_w, self.padding)\n",
    "            if self.data_format == 'channels_first':\n",
    "                output_shape = (\n",
    "                    batch_size, self.filters, out_height, out_width\n",
    "                )\n",
    "            else:\n",
    "                output_shape = (\n",
    "                    batch_size, out_height, out_width, self.filters\n",
    "                )\n",
    "        else:\n",
    "            output_shape = (batch_size,) + self._output_shape\n",
    "\n",
    "        outputs = K.conv2d_transpose(\n",
    "            inputs,\n",
    "            self.kernel,\n",
    "            output_shape,\n",
    "            self.strides,\n",
    "            padding=self.padding,\n",
    "            data_format=self.data_format\n",
    "        )\n",
    "\n",
    "        if self.bias:\n",
    "            outputs = K.bias_add(\n",
    "                outputs, self.bias, data_format=self.data_format\n",
    "            )\n",
    "\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = list(input_shape)\n",
    "        if self.data_format == 'channels_first':\n",
    "            c_axis, h_axis, w_axis = 1, 2, 3\n",
    "        else:\n",
    "            c_axis, h_axis, w_axis = 3, 1, 2\n",
    "\n",
    "        kernel_h, kernel_w = self.kernel_size\n",
    "        stride_h, stride_w = self.strides\n",
    "\n",
    "        if self._output_shape is None:\n",
    "            output_shape[c_axis] = self.filters\n",
    "            output_shape[h_axis] = deconv_length(\n",
    "                output_shape[h_axis], stride_h, kernel_h, self.padding\n",
    "            )\n",
    "            output_shape[w_axis] = deconv_length(\n",
    "                output_shape[w_axis], stride_w, kernel_w, self.padding\n",
    "            )\n",
    "        else:\n",
    "            output_shape[1:] = self._output_shape\n",
    "\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Conv2DTranspose, self).get_config()\n",
    "        config.pop('dilation_rate')\n",
    "        config['output_shape'] = self._output_shape\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Activation, Add, Input, Softmax\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.backend import int_shape, is_keras_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkNet():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        input_tensor=None,\n",
    "        input_shape=None,\n",
    "        initial_block_filters=64,\n",
    "        bias=False,\n",
    "        name='linknet',\n",
    "        \n",
    "    ):\n",
    "        self.num_classes = num_classes\n",
    "        self.initial_block_filters = initial_block_filters\n",
    "        self.bias = bias\n",
    "        self.output_shape = input_shape[:-1] + (num_classes, )\n",
    "\n",
    "         \n",
    "        if input_tensor is None:\n",
    "            self.input = Input(shape=input_shape, name='input_img')\n",
    "        elif is_keras_tensor(input_tensor):\n",
    "            self.input = input_tensor\n",
    "        else:\n",
    "           \n",
    "            self.input = Input(\n",
    "                tensor=input_tensor, shape=input_shape, name='input_img'\n",
    "            )\n",
    "\n",
    "        self.name = name\n",
    "\n",
    "    def get_model(\n",
    "        self,\n",
    "        pretrained_encoder=True,\n",
    "        weights_path='./checkpoints/linknet_encoder_weights.h5'\n",
    "    ):\n",
    "\n",
    "        \n",
    "        encoder_model = self.get_encoder()\n",
    "        if pretrained_encoder:\n",
    "            encoder_model.load_weights(weights_path)\n",
    "        encoder_out = encoder_model(self.input)\n",
    "\n",
    "       \n",
    "        decoder_model = self.get_decoder(encoder_out)\n",
    "        decoder_out = decoder_model(encoder_out[:-1])\n",
    "\n",
    "        return Model(inputs=self.input, outputs=decoder_out, name=self.name)\n",
    "\n",
    "    def get_encoder(self, name='encoder'):\n",
    "\n",
    "       \n",
    "        initial1 = Conv2D(\n",
    "            self.initial_block_filters,\n",
    "            kernel_size=7,\n",
    "            strides=2,\n",
    "            padding='same',\n",
    "            use_bias=self.bias,\n",
    "            name=name + '_0_conv2d_1'\n",
    "        )(self.input)\n",
    "        initial1 = BatchNormalization(name=name + '_0_bn_1')(initial1)\n",
    "        initial1 = Activation('relu', name=name + '_0_relu_1')(initial1)\n",
    "        initial2 = MaxPooling2D(pool_size=2, name=name + '_0_maxpool_1')(initial1)  \n",
    "\n",
    "       \n",
    "        encoder1 = self.encoder_block(\n",
    "            initial2,\n",
    "            self.initial_block_filters,\n",
    "            strides=1,\n",
    "            bias=self.bias,\n",
    "            name=name + '_1'\n",
    "        )\n",
    "        encoder2 = self.encoder_block(\n",
    "            encoder1,\n",
    "            self.initial_block_filters * 2,\n",
    "            strides=(2, 1),\n",
    "            bias=self.bias,\n",
    "            name=name + '_2'\n",
    "        )\n",
    "        encoder3 = self.encoder_block(\n",
    "            encoder2,\n",
    "            self.initial_block_filters * 4,\n",
    "            strides=(2, 1),\n",
    "            bias=self.bias,\n",
    "            name=name + '_3'\n",
    "        )\n",
    "        encoder4 = self.encoder_block(\n",
    "            encoder3,\n",
    "            self.initial_block_filters * 8,\n",
    "            strides=(2, 1),\n",
    "            bias=self.bias,\n",
    "            name=name + '_4'\n",
    "        )\n",
    "\n",
    "        return Model(\n",
    "            inputs=self.input,\n",
    "            outputs=[\n",
    "                encoder4, encoder3, encoder2, encoder1, initial2, initial1\n",
    "            ],\n",
    "            name=name\n",
    "        )\n",
    "\n",
    "    def encoder_block(\n",
    "        self,\n",
    "        input,\n",
    "        out_filters,\n",
    "        kernel_size=3,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        bias=False,\n",
    "        name=''\n",
    "    ):\n",
    "\n",
    "        assert isinstance(strides, (int, tuple, list)), (\n",
    "            \"expected int, tuple, or list for strides\"\n",
    "        )  \n",
    "        if (isinstance(strides, (tuple, list))):\n",
    "            if len(strides) == 2:\n",
    "                stride_1, stride_2 = strides\n",
    "            else:\n",
    "                raise ValueError(\"expected a list or tuple on length 2\")\n",
    "        else:\n",
    "            stride_1 = strides\n",
    "            stride_2 = strides\n",
    "\n",
    "        x = self.encoder_basic_block(\n",
    "            input,\n",
    "            out_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=stride_1,\n",
    "            padding=padding,\n",
    "            bias=bias,\n",
    "            name=name + '_1'\n",
    "        )\n",
    "\n",
    "        x = self.encoder_basic_block(\n",
    "            x,\n",
    "            out_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=stride_2,\n",
    "            padding=padding,\n",
    "            bias=bias,\n",
    "            name=name + '_2'\n",
    "        )\n",
    "\n",
    "        return x\n",
    "\n",
    "    def encoder_basic_block(\n",
    "        self,\n",
    "        input,\n",
    "        out_filters,\n",
    "        kernel_size=3,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        bias=False,\n",
    "        name=''\n",
    "    ):\n",
    "\n",
    "        residual = input\n",
    "\n",
    "        x = Conv2D(\n",
    "            out_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            use_bias=bias,\n",
    "            name=name + '_main_conv2d_1'\n",
    "        )(input)\n",
    "        x = BatchNormalization(name=name + '_main_bn_1')(x)\n",
    "        x = Activation('relu', name=name + '_main_relu_1')(x)\n",
    "\n",
    "        x = Conv2D(\n",
    "            out_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=1,\n",
    "            padding=padding,\n",
    "            use_bias=bias,\n",
    "            name=name + '_main_conv2d_2'\n",
    "        )(x)\n",
    "        x = BatchNormalization(name=name + '_main_bn_2')(x)\n",
    "\n",
    "        if strides > 1:\n",
    "            residual = Conv2D(\n",
    "                out_filters,\n",
    "                kernel_size=1,\n",
    "                strides=strides,\n",
    "                padding=padding,\n",
    "                use_bias=bias,\n",
    "                name=name + '_res_conv2d_1'\n",
    "            )(residual)\n",
    "            residual = BatchNormalization(name=name + '_res_bn_1')(residual)\n",
    "\n",
    "        x = Add(name=name + '_add')([x, residual])\n",
    "        x = Activation('relu', name=name + '_relu_1')(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_decoder(self, inputs, name='decoder'):\n",
    "\n",
    "       \n",
    "        encoder4 = Input(shape=int_shape(inputs[0])[1:], name='encoder4')\n",
    "        encoder3 = Input(shape=int_shape(inputs[1])[1:], name='encoder3')\n",
    "        encoder2 = Input(shape=int_shape(inputs[2])[1:], name='encoder2')\n",
    "        encoder1 = Input(shape=int_shape(inputs[3])[1:], name='encoder1')\n",
    "        initial2 = Input(shape=int_shape(inputs[4])[1:], name='initial2')\n",
    "        initial1 = inputs[5]\n",
    "\n",
    "        \n",
    "        decoder4 = self.decoder_block(\n",
    "            encoder4,\n",
    "            self.initial_block_filters * 4,\n",
    "            strides=2,\n",
    "            output_shape=int_shape(encoder3)[1:],\n",
    "            bias=self.bias,\n",
    "            name=name + '_4'\n",
    "        )\n",
    "        decoder4 = Add(name=name + '_shortcut_e3_d4')([encoder3, decoder4])\n",
    "\n",
    "        decoder3 = self.decoder_block(\n",
    "            decoder4,\n",
    "            self.initial_block_filters * 2,\n",
    "            strides=2,\n",
    "            output_shape=int_shape(encoder2)[1:],\n",
    "            bias=self.bias,\n",
    "            name=name + '_3'\n",
    "        )\n",
    "        decoder3 = Add(name=name + '_shortcut_e2_d3')([encoder2, decoder3])\n",
    "\n",
    "        decoder2 = self.decoder_block(\n",
    "            decoder3,\n",
    "            self.initial_block_filters,\n",
    "            strides=2,\n",
    "            output_shape=int_shape(encoder1)[1:],\n",
    "            bias=self.bias,\n",
    "            name=name + '_2'\n",
    "        )\n",
    "        decoder2 = Add(name=name + '_shortcut_e1_d2')([encoder1, decoder2])\n",
    "\n",
    "        decoder1 = self.decoder_block(\n",
    "            decoder2,\n",
    "            self.initial_block_filters,\n",
    "            strides=1,\n",
    "            output_shape=int_shape(initial2)[1:],\n",
    "            bias=self.bias,\n",
    "            name=name + '_1'\n",
    "        )\n",
    "        decoder1 = Add(name=name + '_shortcut_init_d1')([initial2, decoder1])\n",
    "\n",
    "        shape = (\n",
    "            int_shape(initial1)[1],\n",
    "            int_shape(initial1)[2],\n",
    "            self.initial_block_filters // 2,\n",
    "        )\n",
    "        final = Conv2DTranspose(\n",
    "            self.initial_block_filters // 2,\n",
    "            kernel_size=3,\n",
    "            strides=2,\n",
    "            padding='same',\n",
    "            output_shape=shape,\n",
    "            use_bias=self.bias,\n",
    "            name=name + '_0_transposed2d_1'\n",
    "        )(decoder1)\n",
    "        final = BatchNormalization(name=name + '_0_bn_1')(final)\n",
    "        final = Activation('relu', name=name + '_0_relu_1')(final)\n",
    "\n",
    "        final = Conv2D(\n",
    "            self.initial_block_filters // 2,\n",
    "            kernel_size=3,\n",
    "            padding='same',\n",
    "            use_bias=self.bias,\n",
    "            name=name + '_0_conv2d_1'\n",
    "        )(final)\n",
    "        final = BatchNormalization(name=name + '_0_bn_2')(final)\n",
    "        final = Activation('relu', name=name + '_0_relu_2')(final)\n",
    "\n",
    "        logits = Conv2DTranspose(\n",
    "            self.num_classes,\n",
    "            kernel_size=2,\n",
    "            strides=2,\n",
    "            padding='same',\n",
    "            output_shape=self.output_shape,\n",
    "            use_bias=self.bias,\n",
    "            name=name + '_0_transposed2d_2'\n",
    "        )(final)\n",
    "\n",
    "        \n",
    "        prediction = tf.keras.layers.Activation('sigmoid', name=name + '_0_softmax')(logits) \n",
    "\n",
    "        return Model(\n",
    "            inputs=[\n",
    "                encoder4, encoder3, encoder2, encoder1, initial2\n",
    "            ],\n",
    "            outputs=prediction,\n",
    "            name=name\n",
    "        )\n",
    "\n",
    "    def decoder_block(\n",
    "        self,\n",
    "        input,\n",
    "        out_filters,\n",
    "        kernel_size=3,\n",
    "        strides=2,\n",
    "        projection_ratio=4,\n",
    "        padding='same',\n",
    "        output_shape=None,\n",
    "        bias=False,\n",
    "        name=''\n",
    "    ):\n",
    "        \n",
    "        internal_filters = int_shape(input)[-1] // projection_ratio\n",
    "        x = Conv2D(\n",
    "            internal_filters,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding=padding,\n",
    "            use_bias=bias,\n",
    "            name=name + '_conv2d_1'\n",
    "        )(input)\n",
    "        x = BatchNormalization(name=name + '_bn_1')(x)\n",
    "        x = Activation('relu', name=name + '_relu_1')(x)\n",
    "\n",
    "\n",
    "        shape = output_shape[:-1] + (internal_filters, )\n",
    "        x = Conv2DTranspose(\n",
    "            internal_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            output_shape=shape,\n",
    "            use_bias=bias,\n",
    "            name=name + '_transposed2d_1'\n",
    "        )(x)\n",
    "        x = BatchNormalization(name=name + '_bn_2')(x)\n",
    "        x = Activation('relu', name=name + '_relu_2')(x)\n",
    "\n",
    "        x = Conv2D(\n",
    "            out_filters,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding=padding,\n",
    "            use_bias=bias,\n",
    "            name=name + '_conv2d_2'\n",
    "        )(x)\n",
    "        x = BatchNormalization(name=name + '_bn_3')(x)\n",
    "        x = Activation('relu', name=name + '_relu_3')(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (im_height, im_width, 1)  \n",
    "model = LinkNet(num_classes=1, input_shape=input_shape)\n",
    "model = model.get_model(\n",
    "            pretrained_encoder=False\n",
    "        )\n",
    "model.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy',  \n",
    "                  metrics=[mean_iou_metric])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopper = EarlyStopping(patience=10, verbose=2)\n",
    "checkpointer = ModelCheckpoint('LinkNet.keras', verbose=1, save_best_only=True)\n",
    "results = model.fit(X_train, Y_train, validation_split=0.2, batch_size=16, epochs=150, \n",
    "                    callbacks=[earlystopper, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6)) \n",
    "\n",
    "# Loss Plot\n",
    "ax1.plot(results.history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
    "ax1.set_xlabel('Epochs', fontsize=14)\n",
    "ax1.set_ylabel('Loss', fontsize=14)\n",
    "ax1.set_title('Training and Validation Loss', fontsize=16)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.grid(True)\n",
    "ax1.legend(fontsize=12, loc='upper right')\n",
    "\n",
    "# Mean IoU Plot\n",
    "ax2.plot(results.history['mean_iou'], label='Training Mean IoU', color='green', linewidth=2)\n",
    "ax2.plot(results.history['val_mean_iou'], label='Validation Mean IoU', color='purple', linewidth=2, linestyle='--')\n",
    "ax2.set_xlabel('Epochs', fontsize=14)\n",
    "ax2.set_ylabel('Mean IoU', fontsize=14)\n",
    "ax2.set_title('Training and Validation Mean IoU', fontsize=16)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(True)\n",
    "ax2.legend(fontsize=12, loc='lower right')\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.savefig('Linknet.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.zeros((len(test_ids), im_height, im_width, im_chan), dtype=np.uint8)\n",
    "Y_test = np.zeros((len(test_ids), im_height, im_width, 1), dtype=bool) \n",
    "\n",
    "\n",
    "\n",
    "print('Getting and resizing test images and masks ... ')\n",
    "sys.stdout.flush()\n",
    "\n",
    "for n, id_ in tqdm_notebook(enumerate(test_ids), total=len(test_ids)):\n",
    "    path = path_test\n",
    "    img = load_img(path + '/images/' + id_)\n",
    "    x = img_to_array(img)[:,:,1]\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    X_test[n] = x\n",
    "\n",
    "    mask = img_to_array(load_img(path + '/masks/' + id_))[:,:,1]\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    Y_test[n] = mask\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\n",
    "    '/kaggle/working/LinkNet.keras',\n",
    "    custom_objects={'MeanIoUMetric': MeanIoUMetric} ,safe_mode=False\n",
    ")\n",
    "\n",
    "\n",
    "preds_test = model.predict(X_test, verbose=1)\n",
    "preds_test_t = (preds_test > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform a sanity check on some random Test Samples\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    " \n",
    "plt.style.use('seaborn-whitegrid') \n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))  \n",
    "fig.subplots_adjust(hspace=0.3, wspace=-0.5)\n",
    "\n",
    "for i in range(3):\n",
    "    if i ==0 :\n",
    "        axes[i, 0].set_title(\"Image\", fontsize=16) \n",
    "        axes[i, 1].set_title(\"Ground Truth\", fontsize=16)\n",
    "        axes[i, 2].set_title(\"Predicted\", fontsize=16)\n",
    "        \n",
    "        \n",
    "    ix = random.randint(0, len(preds_test_t) - 1) \n",
    "\n",
    "    # Image\n",
    "    axes[i, 0].imshow(np.dstack((X_test[ix], X_test[ix], X_test[ix])))\n",
    "    \n",
    "    axes[i, 0].axis('off') \n",
    "\n",
    "    \n",
    "    # Ground Truth\n",
    "    im_gt = axes[i, 1].imshow(Y_test[ix], cmap='magma')\n",
    "    \n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "\n",
    "    \n",
    "    # Prediction\n",
    "    im_pred = axes[i, 2].imshow(preds_test_t[ix], cmap='magma')\n",
    "    \n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "\n",
    "plt.savefig(\"prediction_visualization.png\", dpi=600) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
