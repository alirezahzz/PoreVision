{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Install Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation-models-pytorch\n",
    "!pip install pytorch-lightning==1.8.3.post0\n",
    "!pip install torchsummary\n",
    "!pip install vit-pytorch\n",
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prevent GPU from Memory Growth\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate \n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.utils import register_keras_serializable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Define Mean IoU Metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "class MeanIoUMetric(metrics.Metric):\n",
    "    \n",
    "    def __init__(self, num_classes, name='mean_iou', **kwargs):\n",
    "        super(MeanIoUMetric, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes \n",
    "        self.iou_metric = metrics.MeanIoU(num_classes=num_classes) \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_ = tf.cast(y_pred > 0.5, tf.int32)  \n",
    "        self.iou_metric.update_state(y_true, y_pred_)\n",
    "\n",
    "    def result(self):\n",
    "        return self.iou_metric.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.iou_metric.reset_state()\n",
    "        \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)  \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_classes\": self.num_classes})\n",
    "        return config\n",
    "    \n",
    "    \n",
    "mean_iou_metric = MeanIoUMetric(num_classes=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Supervised = tf.keras.models.load_model(\n",
    "    '/kaggle/working/unetplusplus-save-model-keras/UNET_plusplus.keras',\n",
    "    custom_objects={'MeanIoUMetric': MeanIoUMetric} ,safe_mode=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load Checkboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.swin_transformer import SwinTransformer\n",
    "from vit_pytorch import ViT\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from segmentation_models_pytorch import UnetPlusPlus\n",
    "\n",
    "\n",
    "all_attention_loss  = []\n",
    "\n",
    "def random_checkboard_mask_new(img, ratio_n=None):\n",
    "\n",
    "    if ratio_n == None:\n",
    "        random_value = torch.rand(1)\n",
    "\n",
    "    if random_value < 1/6:\n",
    "        mask = np.load(\"/kaggle/input/ck-mask/ck_mask/ck_0.npy\")\n",
    "\n",
    "    elif 1/6 < random_value < 2/6:\n",
    "        mask = np.load(\"/kaggle/input/ck-mask/ck_mask/ck_1.npy\")\n",
    "\n",
    "    elif 2/6 < random_value < 3/6:\n",
    "        mask = np.load(\"/kaggle/input/ck-mask/ck_mask/ck_2.npy\")\n",
    "\n",
    "    elif 3/6 < random_value < 4/6:\n",
    "        mask = np.load(\"/kaggle/input/ck-mask/ck_mask/ck_3.npy\")\n",
    "\n",
    "    elif 4/6 < random_value < 5/6:\n",
    "        mask = np.load(\"/kaggle/input/ck-mask/ck_mask/ck_4.npy\")\n",
    "\n",
    "    else:\n",
    "        mask = np.load(\"/kaggle/input/ck-mask/ck_mask/ck_5.npy\")\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "import cv2 as cv\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, image_dir, subfolder='train', direction='AtoB', transform=None, resize_scale=None, crop_size=None, fliplr=False):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        if direction == 'AtoB':\n",
    "            self.input_path = os.path.join(image_dir, subfolder, 'a')\n",
    "            self.target_path = os.path.join(image_dir, subfolder, 'b')\n",
    "        else:\n",
    "            self.input_path = os.path.join(image_dir, subfolder, 'b')\n",
    "            self.target_path = os.path.join(image_dir, subfolder, 'a')\n",
    "\n",
    "        self.image_filenames = [x for x in sorted(os.listdir(self.input_path))]\n",
    "        #self.image_filenames = self.image_filenames[:200]\n",
    "\n",
    "        \n",
    "     \n",
    "        self.direction = direction\n",
    "        self.transform = transform\n",
    "        self.resize_scale = resize_scale    \n",
    "        self.crop_size = crop_size \n",
    "        self.fliplr = fliplr    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \n",
    "        img_fn = os.path.join(self.input_path, self.image_filenames[index])\n",
    "        img_tar = os.path.join(self.target_path, self.image_filenames[index])\n",
    "        img_input = cv.imread(img_fn)\n",
    "        img_target = cv.imread(img_tar)\n",
    "        img_target2 = cv.imread(img_tar)\n",
    "        \n",
    "        stride = False\n",
    "        \n",
    "        if stride:\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            mask1 = random_checkboard_mask_new(img_input, None)\n",
    "        \n",
    "        img_input = cv.resize(img_input, (256, 256))\n",
    "        img_input = img_input * mask1\n",
    "        \n",
    "        \n",
    "\n",
    "        if self.resize_scale:\n",
    "            img_input = cv.resize(img_input, (self.resize_scale, self.resize_scale))\n",
    "            img_target = cv.resize(img_target, (self.resize_scale, self.resize_scale))\n",
    "\n",
    "\n",
    "        if self.crop_size:\n",
    "            \n",
    "            x = random.randint(0, self.resize_scale - self.crop_size)\n",
    "            y = random.randint(0, self.resize_scale - self.crop_size)\n",
    "            \n",
    "            img_input = img_input[x : x + self.crop_size, y:y+self.crop_size, :]\n",
    "            img_target = img_target[x : x + self.crop_size, y:y+self.crop_size, :]\n",
    "\n",
    "\n",
    "        if self.fliplr:\n",
    "            if random.random() < 0.5:\n",
    "                \n",
    "                img_input = cv.flip(img_input, 1)\n",
    "                img_target = cv.flip(img_target, 1)\n",
    "\n",
    "        img_input = transforms.ToPILImage()(img_input)\n",
    "        img_target = transforms.ToPILImage()(img_target)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img_input = self.transform(img_input)\n",
    "            img_target = self.transform(img_target)\n",
    "        img_target2 = cv.resize(img_target2, (256, 256))\n",
    "        return img_input, img_target, mask1 ,img_target2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Generator and  Discriminator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_filter, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.model = UnetPlusPlus(\n",
    "            encoder_name=\"resnet34\", \n",
    "            encoder_weights=\"imagenet\", \n",
    "            in_channels=input_dim,         \n",
    "            classes=output_dim,            \n",
    "            decoder_use_batchnorm=True\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class SwinTDiscriminator(nn.Module):\n",
    "    def __init__(self, image_size=256, patch_size=4, num_classes=1):\n",
    "        super(SwinTDiscriminator, self).__init__()\n",
    "        self.swin_t = SwinTransformer(\n",
    "            img_size=image_size, \n",
    "            patch_size=patch_size, \n",
    "            in_chans=6, \n",
    "            num_classes=num_classes, \n",
    "            embed_dim=48, \n",
    "            depths=[1, 1, 3, 1], \n",
    "            num_heads=[8,16,32,64], \n",
    "            window_size=8, \n",
    "            mlp_ratio=2., \n",
    "            qkv_bias=True, \n",
    "            qk_scale=None, \n",
    "            drop_rate=0., \n",
    "            attn_drop_rate=0., \n",
    "            drop_path_rate=0.1, \n",
    "            norm_layer=nn.LayerNorm, \n",
    "            ape=False, \n",
    "            patch_norm=True, \n",
    "            use_checkpoint=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        x = torch.cat([x, label], 1)  \n",
    "        x = self.swin_t(x)  \n",
    "        x = torch.sigmoid(x) \n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import imageio\n",
    "import cv2 as cv\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def to_np(x):\n",
    "    return x.data.cpu().numpy()\n",
    "\n",
    "\n",
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "\n",
    "def normalization(data):\n",
    "    _range = np.max(data) - np.min(data)\n",
    "    return (data - np.min(data)) / _range\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from StringIO import StringIO  # Python 2.7\n",
    "except ImportError:\n",
    "    from io import BytesIO  # Python 3.x\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, log_dir):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.FileWriter(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSGMS_Loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "import kornia\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "class Prewitt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.filter = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "        Gx = torch.tensor([[-1.0, 0.0, 1.0], [-1.0, 0.0, 1.0], [-1.0, 0.0, 1.0]]) / 3\n",
    "        Gy = torch.tensor([[1.0, 1.0, 1.0], [0.0, 0.0, 0.0], [-1.0, -1.0, -1.0]]) / 3\n",
    "        G = torch.cat([Gx.unsqueeze(0), Gy.unsqueeze(0)], 0)\n",
    "        G = G.unsqueeze(1).to(device)\n",
    "        self.filter.weight = nn.Parameter(G, requires_grad=False)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.filter(img)\n",
    "        x = torch.mul(x, x)\n",
    "        x = torch.sum(x, dim=1, keepdim=True)\n",
    "        x = torch.sqrt(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the gradient magnitude similarity map:\n",
    "def GMS(Ii, Ir, edge_filter, median_filter, c=0.0026):\n",
    "    x = torch.mean(Ii, dim=1, keepdim=True)\n",
    "    y = torch.mean(Ir, dim=1, keepdim=True)\n",
    "    g_I = edge_filter(median_filter(x))\n",
    "    g_Ir = edge_filter(median_filter(y))\n",
    "    g_map = (2 * g_I * g_Ir + c) / (g_I**2 + g_Ir**2 + c)\n",
    "    return g_map\n",
    "\n",
    "\n",
    "class MSGMS_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.GMS = partial(GMS, edge_filter=Prewitt(), median_filter=kornia.filters.MedianBlur((3, 3)))\n",
    "\n",
    "    def GMS_loss(self, Ii, Ir):\n",
    "        return torch.mean(1 - self.GMS(Ii, Ir))\n",
    "\n",
    "    def forward(self, Ii, Ir):\n",
    "        total_loss = self.GMS_loss(Ii, Ir)\n",
    "\n",
    "        for _ in range(3):\n",
    "            Ii = F.avg_pool2d(Ii, kernel_size=2, stride=2)\n",
    "            Ir = F.avg_pool2d(Ir, kernel_size=2, stride=2)\n",
    "            total_loss += self.GMS_loss(Ii, Ir)\n",
    "\n",
    "        return total_loss / 4\n",
    "\n",
    "\n",
    "class MSGMS_Score(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.GMS = partial(GMS, edge_filter=Prewitt(), median_filter=kornia.filters.MedianBlur((3, 3)))\n",
    "        self.median_filter = kornia.filters.MedianBlur((21, 21))\n",
    "\n",
    "    def GMS_Score(self, Ii, Ir):\n",
    "        return self.GMS(Ii, Ir)\n",
    "\n",
    "    def forward(self, Ii, Ir):\n",
    "        total_scores = self.GMS_Score(Ii, Ir)\n",
    "        img_size = Ii.size(-1)\n",
    "        total_scores = F.interpolate(total_scores, size=img_size, mode='bilinear', align_corners=False)\n",
    "        for _ in range(3):\n",
    "            Ii = F.avg_pool2d(Ii, kernel_size=2, stride=2)\n",
    "            Ir = F.avg_pool2d(Ir, kernel_size=2, stride=2)\n",
    "            score = self.GMS_Score(Ii, Ir)\n",
    "            total_scores += F.interpolate(score, size=img_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "        return (1 - total_scores) / 4\n",
    "\n",
    "\n",
    "#PS_LOSS\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.add_module('vgg', VGG19().cuda())\n",
    "        self.criterion = torch.nn.L1Loss()\n",
    "\n",
    "    def compute_gram(self, x):\n",
    "        b, ch, h, w = x.size()\n",
    "        f = x.view(b, ch, w * h)\n",
    "        f_T = f.transpose(1, 2)\n",
    "        G = f.bmm(f_T) / (h * w * ch)\n",
    "\n",
    "        return G\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        # Compute features\n",
    "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
    "\n",
    "        # Compute loss\n",
    "        style_loss = 0.0\n",
    "        style_loss += self.criterion(self.compute_gram(x_vgg['relu2_2']), self.compute_gram(y_vgg['relu2_2']))\n",
    "        style_loss += self.criterion(self.compute_gram(x_vgg['relu3_4']), self.compute_gram(y_vgg['relu3_4']))\n",
    "        style_loss += self.criterion(self.compute_gram(x_vgg['relu4_4']), self.compute_gram(y_vgg['relu4_4']))\n",
    "        style_loss += self.criterion(self.compute_gram(x_vgg['relu5_2']), self.compute_gram(y_vgg['relu5_2']))\n",
    "\n",
    "        return style_loss\n",
    "\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    r\"\"\"\n",
    "    Perceptual loss, VGG-based\n",
    "    https://arxiv.org/abs/1603.08155\n",
    "    https://github.com/dxyang/StyleTransfer/blob/master/utils.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights=[1.0, 1.0, 1.0, 1.0, 1.0]):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.add_module('vgg', VGG19().cuda())\n",
    "        self.criterion = torch.nn.L1Loss().cuda()\n",
    "        self.weights = weights\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        # Compute features\n",
    "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
    "\n",
    "        content_loss = 0.0\n",
    "        content_loss += self.weights[0] * self.criterion(x_vgg['relu1_1'], y_vgg['relu1_1'])\n",
    "        content_loss += self.weights[1] * self.criterion(x_vgg['relu2_1'], y_vgg['relu2_1'])\n",
    "        content_loss += self.weights[2] * self.criterion(x_vgg['relu3_1'], y_vgg['relu3_1'])\n",
    "        content_loss += self.weights[3] * self.criterion(x_vgg['relu4_1'], y_vgg['relu4_1'])\n",
    "        content_loss += self.weights[4] * self.criterion(x_vgg['relu5_1'], y_vgg['relu5_1'])\n",
    "\n",
    "\n",
    "        return content_loss\n",
    "\n",
    "\n",
    "\n",
    "class VGG19(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19, self).__init__()\n",
    "        features = models.vgg19(pretrained=True).features\n",
    "        self.relu1_1 = torch.nn.Sequential()\n",
    "        self.relu1_2 = torch.nn.Sequential()\n",
    "\n",
    "        self.relu2_1 = torch.nn.Sequential()\n",
    "        self.relu2_2 = torch.nn.Sequential()\n",
    "\n",
    "        self.relu3_1 = torch.nn.Sequential()\n",
    "        self.relu3_2 = torch.nn.Sequential()\n",
    "        self.relu3_3 = torch.nn.Sequential()\n",
    "        self.relu3_4 = torch.nn.Sequential()\n",
    "\n",
    "        self.relu4_1 = torch.nn.Sequential()\n",
    "        self.relu4_2 = torch.nn.Sequential()\n",
    "        self.relu4_3 = torch.nn.Sequential()\n",
    "        self.relu4_4 = torch.nn.Sequential()\n",
    "\n",
    "        self.relu5_1 = torch.nn.Sequential()\n",
    "        self.relu5_2 = torch.nn.Sequential()\n",
    "        self.relu5_3 = torch.nn.Sequential()\n",
    "        self.relu5_4 = torch.nn.Sequential()\n",
    "\n",
    "        for x in range(2):\n",
    "            \n",
    "            self.relu1_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(2, 4):\n",
    "            self.relu1_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(4, 7):\n",
    "            self.relu2_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(7, 9):\n",
    "            self.relu2_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(9, 12):\n",
    "            self.relu3_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(12, 14):\n",
    "            self.relu3_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(14, 16):\n",
    "            self.relu3_3.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(16, 18):\n",
    "            self.relu3_4.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(18, 21):\n",
    "            self.relu4_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(21, 23):\n",
    "            self.relu4_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(23, 25):\n",
    "            self.relu4_3.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(25, 27):\n",
    "            self.relu4_4.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(27, 30):\n",
    "            self.relu5_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(30, 32):\n",
    "            self.relu5_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(32, 34):\n",
    "            self.relu5_3.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(34, 36):\n",
    "            self.relu5_4.add_module(str(x), features[x])\n",
    "\n",
    "        # don't need the gradients, just want the features\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        relu1_1 = self.relu1_1(x)\n",
    "        relu1_2 = self.relu1_2(relu1_1)\n",
    "\n",
    "        relu2_1 = self.relu2_1(relu1_2)\n",
    "        relu2_2 = self.relu2_2(relu2_1)\n",
    "\n",
    "        relu3_1 = self.relu3_1(relu2_2)\n",
    "        relu3_2 = self.relu3_2(relu3_1)\n",
    "        relu3_3 = self.relu3_3(relu3_2)\n",
    "        relu3_4 = self.relu3_4(relu3_3)\n",
    "\n",
    "        relu4_1 = self.relu4_1(relu3_4)\n",
    "        relu4_2 = self.relu4_2(relu4_1)\n",
    "        relu4_3 = self.relu4_3(relu4_2)\n",
    "        relu4_4 = self.relu4_4(relu4_3)\n",
    "\n",
    "        relu5_1 = self.relu5_1(relu4_4)\n",
    "        relu5_2 = self.relu5_2(relu5_1)\n",
    "        relu5_3 = self.relu5_3(relu5_2)\n",
    "        relu5_4 = self.relu5_4(relu5_3)\n",
    "\n",
    "        out = {\n",
    "            'relu1_1': relu1_1,\n",
    "            'relu1_2': relu1_2,\n",
    "\n",
    "            'relu2_1': relu2_1,\n",
    "            'relu2_2': relu2_2,\n",
    "\n",
    "            'relu3_1': relu3_1,\n",
    "            'relu3_2': relu3_2,\n",
    "            'relu3_3': relu3_3,\n",
    "            'relu3_4': relu3_4,\n",
    "\n",
    "            'relu4_1': relu4_1,\n",
    "            'relu4_2': relu4_2,\n",
    "            'relu4_3': relu4_3,\n",
    "            'relu4_4': relu4_4,\n",
    "\n",
    "            'relu5_1': relu5_1,\n",
    "            'relu5_2': relu5_2,\n",
    "            'relu5_3': relu5_3,\n",
    "            'relu5_4': relu5_4,\n",
    "        }\n",
    "        return out\n",
    "\n",
    "\n",
    "        \n",
    "# SSIM:\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
    "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
    "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1*mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01**2\n",
    "    C2 = 0.03**2\n",
    "\n",
    "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "class SSIM(torch.nn.Module):\n",
    "    def __init__(self, window_size = 11, size_average = True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "            \n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "            \n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "\n",
    "def ssim(img1, img2, window_size = 11, size_average = True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "    \n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "    \n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)\n",
    "\n",
    "#New Pore Attention Loss\n",
    "\n",
    "def get_pore_attention_mask(Model_Supervised, image, y_, epoch):\n",
    "\n",
    "    batch_size = image.shape[0]  \n",
    "    masks = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        img = image[i, :, :, :].cpu().numpy()\n",
    "        img = cv2.resize(img, (512, 512))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) \n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "\n",
    "        pred_mask = Model_Supervised.predict(img, verbose=0)  \n",
    "        pred_mask = (pred_mask > 0.3).astype(np.uint8)  \n",
    "        pred_mask = resize(pred_mask, (1, 256, 256, 1), mode='constant', preserve_range=True)\n",
    "        masks.append(pred_mask)  \n",
    "\n",
    "    masks = np.concatenate(masks, axis=0) \n",
    " \n",
    "    return masks\n",
    "\n",
    "\n",
    "def attention_guided_l1_loss(gen_image, target_image, attention_mask, epoch):\n",
    "\n",
    "\n",
    "    attention_mask = torch.from_numpy(attention_mask).to('cuda')\n",
    "    attention_mask = attention_mask.permute(0, 3, 1, 2) \n",
    "\n",
    "    masked_gen_image = gen_image * attention_mask\n",
    "    masked_target_image = target_image * attention_mask\n",
    "    masked_l1_loss = L1_loss(masked_gen_image, masked_target_image)\n",
    "\n",
    "    \n",
    "    masked_gen_image = masked_gen_image.cpu().detach().numpy()  \n",
    "    masked_target_image = masked_target_image.cpu().detach().numpy() \n",
    "\n",
    "#     if (epoch+1) % 5 == 0:\n",
    "#         plt.figure(figsize=(10, 5))\n",
    "#         plt.subplot(1, 3, 1)\n",
    "#         plt.imshow(masked_gen_image[0,0,:,:], cmap='gray') \n",
    "#         plt.title(\" gen_image * attention_mask\")\n",
    "\n",
    "#         plt.subplot(1, 3, 2)\n",
    "#         plt.imshow(masked_target_image[0,0,:,:], cmap='gray') \n",
    "#         plt.title(\"target_image * attention_mask\")\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "    return masked_l1_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from math import exp\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize \n",
    "import time\n",
    "\n",
    "\n",
    "direction = 'AtoB'\n",
    "batch_size= 16\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "input_size = 256\n",
    "resize_scale = None\n",
    "crop_size = None\n",
    "fliplr = False\n",
    "num_epochs = 100\n",
    "lrG = 0.0008\n",
    "lrD = 0.0002\n",
    "lamb = 100.0\n",
    "beta1 = 0.5\n",
    "beta2= 0.999\n",
    "writer = SummaryWriter('./path/log1')\n",
    "data_dir = '/kaggle/input/path dataset'\n",
    "model_dir = './saved-model/'\n",
    "\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(input_size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "train_data = DatasetFromFolder(data_dir, subfolder='train', direction=direction, \n",
    "                resize_scale=resize_scale,  transform=transform, crop_size=crop_size, fliplr=fliplr)\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True, pin_memory=True, num_workers=72, prefetch_factor=20, persistent_workers=True)\n",
    "\n",
    "test_data = DatasetFromFolder(data_dir, subfolder='validation', direction=direction, transform=transform)\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True)\n",
    "\n",
    "\n",
    "G = Generator(3, ngf, 3)\n",
    "D = SwinTDiscriminator()\n",
    "if torch.cuda.device_count() > 1:  # If we have multiple GPUs\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    D = nn.DataParallel(D)\n",
    "    G = nn.DataParallel(G)\n",
    "D.cuda()\n",
    "G.cuda()\n",
    "\n",
    "\n",
    "BCE_loss = torch.nn.BCELoss().cuda()\n",
    "L1_loss = torch.nn.L1Loss().cuda()\n",
    "L2_loss = torch.nn.MSELoss().cuda()\n",
    "\n",
    "\n",
    "perceptual_loss = PerceptualLoss().cuda()\n",
    "style_loss = StyleLoss().cuda()\n",
    "msgms_loss = MSGMS_Loss().cuda()\n",
    "\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lrG, betas=(beta1, beta2))\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lrD, betas=(beta1, beta2))\n",
    "\n",
    "def adjust_learning_rate1(optimizer, epoch):\n",
    "    lr = 0.0001*(0.99**(epoch))\n",
    "    print(\"lr is {}\".format(lr))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def adjust_learning_rate2(optimizer, epoch):\n",
    "    lr = 0.0004*(0.99**(epoch))\n",
    "    print(\"lr is {}\".format(lr))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "D_avg_losses = []\n",
    "G_avg_losses = []\n",
    "\n",
    "step = 0\n",
    "\n",
    "loss_L1 = False\n",
    "loss_L1_Style = False\n",
    "loss_L1_SSIM_GMS = False\n",
    "loss_L1_SSIM_GMS_Style = True\n",
    "\n",
    "best_val_loss = 10000000000\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('./path/log1')\n",
    "\n",
    "# Initialize SSIM function\n",
    "ssim_func = SSIM().cuda()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    time1 = time.time()\n",
    "    D_losses = []\n",
    "    G_losses = []\n",
    "    adjust_learning_rate1(G_optimizer, epoch)\n",
    "    adjust_learning_rate2(D_optimizer, epoch)\n",
    "    train_ssim_values = []\n",
    "    for i, (input, target, mask,img_target2) in enumerate(train_data_loader):\n",
    "        \n",
    "        x_ = Variable(input.cuda())\n",
    "        y_ = Variable(target.cuda())\n",
    "        \n",
    "    \n",
    "        D_real_decision = D(x_, y_).squeeze()\n",
    "        real_ = Variable(torch.ones(D_real_decision.size()).cuda())\n",
    "        D_real_loss = BCE_loss(D_real_decision, real_)\n",
    "    \n",
    "        gen_image = G(x_)\n",
    "        D_fake_decision = D(x_, gen_image).squeeze()\n",
    "        fake_ = Variable(torch.zeros(D_fake_decision.size()).cuda())\n",
    "        D_fake_loss = BCE_loss(D_fake_decision, fake_)\n",
    "    \n",
    "        D_loss = (D_real_loss + D_fake_loss) * 0.5\n",
    "        D_optimizer.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "    \n",
    "        gen_image = G(x_)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ssim_value = ssim_func(gen_image, y_)\n",
    "            train_ssim_values.append(ssim_value.item())\n",
    "        \n",
    "        attention_mask = get_pore_attention_mask(Model_Supervised,img_target2, y_ , epoch)\n",
    "        attention_loss = attention_guided_l1_loss(gen_image, y_, attention_mask,epoch)\n",
    "    \n",
    "        \n",
    "        D_fake_decision = D(x_, gen_image).squeeze()\n",
    "        G_fake_loss = BCE_loss(D_fake_decision, real_) # fool the discriminator into classifying the generated image as real.\n",
    "          \n",
    "        if loss_L1 == True:\n",
    "            # using pure L1 loss\n",
    "            l1_loss = lamb * L1_loss(gen_image, y_)\n",
    "        elif loss_L1_SSIM_GMS_Style == True:\n",
    "            loss_MSGMS = msgms_loss(gen_image, y_)\n",
    "            loss_SSIM = 1 - ssim(gen_image, y_)\n",
    "            gen_style_loss = style_loss(gen_image, y_) * 10\n",
    "            l_rec = gen_style_loss + loss_MSGMS + loss_SSIM + L1_loss(gen_image, y_)\n",
    "            l1_loss = lamb * l_rec\n",
    "            \n",
    "        \n",
    "        G_loss = G_fake_loss + l1_loss + attention_loss \n",
    "        all_attention_loss.append(attention_loss)\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        # loss values\n",
    "        D_losses.append(D_loss.item())\n",
    "        G_losses.append(G_loss.item())\n",
    "        \n",
    "        print('Epoch [%d/%d], Step [%d/%d], D_loss: %.4f, G_loss: %.4f '\n",
    "              % (epoch+1, num_epochs, i+1, len(train_data_loader), D_loss.item(), G_loss.item()))\n",
    "        step += 1\n",
    "        \n",
    "    D_avg_loss = torch.mean(torch.FloatTensor(D_losses))\n",
    "    G_avg_loss = torch.mean(torch.FloatTensor(G_losses))\n",
    "    \n",
    "    D_avg_losses.append(D_avg_loss)\n",
    "    G_avg_losses.append(G_avg_loss)\n",
    "    \n",
    "    avg_train_ssim = sum(train_ssim_values) / len(train_ssim_values)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train SSIM: {avg_train_ssim:.4f}')\n",
    "    \n",
    "    writer.add_scalar('G_loss_mean', torch.mean(torch.FloatTensor(G_losses)), epoch)\n",
    "    writer.add_scalar('D_loss_mean', torch.mean(torch.FloatTensor(D_losses)), epoch)\n",
    "\n",
    "    time2 = time.time()\n",
    "    print(\"Time for Each Epoch is {}\".format(time2 - time1))\n",
    "    \n",
    "    if (epoch+1) % 2 == 0: \n",
    "        val_losses = 0.00\n",
    "        # time_start = time.time()\n",
    "        for i, (input, target, mask,img_target2) in enumerate(test_data_loader):\n",
    "            x_ = Variable(input.cuda())\n",
    "            y_ = Variable(target.cuda())\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                gen_image = G(x_)\n",
    "            if loss_L1 == True:\n",
    "            # using pure L1 loss\n",
    "                l1_loss =  L1_loss(gen_image, y_)\n",
    "            elif loss_L1_SSIM_GMS_Style == True:\n",
    "                loss_MSGMS = msgms_loss(gen_image, y_)\n",
    "                loss_SSIM = 1 - ssim(gen_image, y_)\n",
    "                gen_style_loss = style_loss(gen_image, y_) * 10\n",
    "                l_rec = gen_style_loss + loss_MSGMS + loss_SSIM + L1_loss(gen_image, y_)\n",
    "                l1_loss = lamb * l_rec\n",
    "\n",
    "            loss_all = l1_loss\n",
    "            val_losses += loss_all\n",
    "\n",
    "        if val_losses < best_val_loss:\n",
    "            best_val_loss = min(best_val_loss, val_losses)\n",
    "            print(\"best_val_loss is {}\".format(best_val_loss))\n",
    "            torch.save(G.state_dict(), model_dir + 'best_G_param.pkl')\n",
    "            torch.save(D.state_dict(), model_dir + 'best_D_param.pkl')\n",
    "            print(\"the best model is epoch_{}\".format(epoch + 1))\n",
    "\n",
    "\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        torch.save(G.state_dict(), model_dir + '%d'%(epoch +1) +'generator_param.pkl')\n",
    "        torch.save(D.state_dict(), model_dir + '%d'%(epoch +1) + 'discriminator_param.pkl')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_ssim = range(len(train_ssim_values))  \n",
    "# Plot SSIM \n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_ssim, train_ssim_values, label='SSIM', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('SSIM Value')\n",
    "plt.title('Structural Similarity Index (SSIM) During Training')\n",
    "plt.legend()\n",
    "#plt.grid(True)\n",
    "plt.savefig(\"Structural Similarity Index.png\", dpi=600) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "import cv2 as cv\n",
    "from torchvision import datasets, transforms\n",
    "from segmentation_models_pytorch import UnetPlusPlus\n",
    "from torchsummary import summary\n",
    "from timm.models.swin_transformer import SwinTransformer\n",
    "from vit_pytorch import ViT\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "\n",
    "data_dir = '/kaggle/input/path dataset'\n",
    "save_error_dir = '/kaggle/working/result/'\n",
    "\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "\n",
    "def random_checkboard_mask_new(img, ratio_n=None):\n",
    "\n",
    "    if ratio_n == None:\n",
    "        random_value = torch.rand(1)\n",
    "\n",
    "    if random_value < 1/6:\n",
    "        mask = np.load(\"/kaggle/input/ck-mask/ck_mask/ck_0.npy\")\n",
    "\n",
    "    elif 1/6 < random_value < 2/6:\n",
    "        mask = np.load(\"/kaggle/input/ck-mask/ck_mask/ck_1.npy\")\n",
    "\n",
    "    elif 2/6 < random_value < 3/6:\n",
    "        mask = np.load(\"/kaggle/input/ck-mask/ck_mask/ck_2.npy\")\n",
    "\n",
    "    elif 3/6 < random_value < 4/6:\n",
    "        mask = np.load(\"/kaggle/input/ck-mask/ck_mask/ck_3.npy\")\n",
    "\n",
    "    elif 4/6 < random_value < 5/6:\n",
    "        mask = np.load(\"/kaggle/input/ck-mask/ck_mask/ck_4.npy\")\n",
    "\n",
    "    else:\n",
    "        mask = np.load(\"/kaggle/input/ck-mask/ck_mask/ck_5.npy\")\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_filter, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.model = UnetPlusPlus(\n",
    "            encoder_name=\"resnet34\", \n",
    "            encoder_weights=\"imagenet\", \n",
    "            in_channels=input_dim,         \n",
    "            classes=output_dim,            \n",
    "            decoder_use_batchnorm=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    \n",
    "class SwinTDiscriminator(nn.Module):\n",
    "    def __init__(self, image_size=256, patch_size=4, num_classes=1):\n",
    "        super(SwinTDiscriminator, self).__init__()\n",
    "        self.swin_t = SwinTransformer(\n",
    "            img_size=image_size, \n",
    "            patch_size=patch_size, \n",
    "            in_chans=6, \n",
    "            num_classes=num_classes, \n",
    "            embed_dim=48, \n",
    "            depths=[1, 1, 3, 1], \n",
    "            num_heads=[8,16,32,64], \n",
    "            window_size=8, \n",
    "            mlp_ratio=2., \n",
    "            qkv_bias=True, \n",
    "            qk_scale=None, \n",
    "            drop_rate=0., \n",
    "            attn_drop_rate=0., \n",
    "            drop_path_rate=0.1, \n",
    "            norm_layer=nn.LayerNorm, \n",
    "            ape=False, \n",
    "            patch_norm=True, \n",
    "            use_checkpoint=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        x = torch.cat([x, label], 1)  \n",
    "        x = self.swin_t(x)  \n",
    "        x = torch.sigmoid(x) \n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, image_dir, subfolder='train', direction='AtoB', transform=None, resize_scale=None, crop_size=None, fliplr=False):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        if direction == 'AtoB':\n",
    "            self.input_path = os.path.join(image_dir, subfolder, 'a')\n",
    "            self.target_path = os.path.join(image_dir, subfolder, 'b')\n",
    "            \n",
    "        else:\n",
    "            self.input_path = os.path.join(image_dir, subfolder, 'b')\n",
    "            self.target_path = os.path.join(image_dir, subfolder, 'a')\n",
    "\n",
    "        self.image_filenames = [x for x in sorted(os.listdir(self.input_path))]\n",
    "        #self.image_filenames = self.image_filenames[:200]\n",
    "\n",
    "        \n",
    "        self.direction = direction\n",
    "        self.transform = transform\n",
    "        self.resize_scale = resize_scale    \n",
    "        self.crop_size = crop_size \n",
    "        self.fliplr = fliplr    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \n",
    "        img_fn = os.path.join(self.input_path, self.image_filenames[index])\n",
    "        img_tar = os.path.join(self.target_path, self.image_filenames[index])\n",
    "        img_input = cv.imread(img_fn)\n",
    "        img_target = cv.imread(img_tar)\n",
    "        img_target2 = cv.imread(img_tar)\n",
    "        \n",
    "        stride = False\n",
    "        \n",
    "        if stride:\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if self.resize_scale:\n",
    "            img_input = cv.resize(img_input, (self.resize_scale, self.resize_scale))\n",
    "            img_target = cv.resize(img_target, (self.resize_scale, self.resize_scale))\n",
    "\n",
    "\n",
    "        if self.crop_size:\n",
    "            \n",
    "            x = random.randint(0, self.resize_scale - self.crop_size)\n",
    "            y = random.randint(0, self.resize_scale - self.crop_size)\n",
    "            \n",
    "            img_input = img_input[x : x + self.crop_size, y:y+self.crop_size, :]\n",
    "            img_target = img_target[x : x + self.crop_size, y:y+self.crop_size, :]\n",
    "\n",
    "\n",
    "        if self.fliplr:\n",
    "            if random.random() < 0.5:\n",
    "                \n",
    "                img_input = cv.flip(img_input, 1)\n",
    "                img_target = cv.flip(img_target, 1)\n",
    "\n",
    "        img_input = transforms.ToPILImage()(img_input)\n",
    "        img_target = transforms.ToPILImage()(img_target)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img_input = self.transform(img_input)\n",
    "            img_target = self.transform(img_target)\n",
    "        img_target2 = cv.resize(img_target2, (256, 256))\n",
    "        return img_input, img_target ,img_target2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    \n",
    "def save_error_maps_gray(input_name, input_ori, target, gen_image,\n",
    "                                  epoch, save=False, save_dir='output_results/'):\n",
    "\n",
    "    name = input_name.split(\".\")[0]\n",
    "\n",
    "    gen_image_show = (((gen_image[0] - gen_image[0].min()) * 255) / (\n",
    "                 gen_image[0].max() - gen_image[0].min())).numpy().transpose(1, 2, 0).astype(np.uint8)\n",
    "    input_image_show = (((input_ori[0] - input_ori[0].min()) * 255) / (\n",
    "                 input_ori[0].max() - input_ori[0].min())).numpy().transpose(1, 2, 0).astype(np.uint8)\n",
    "\n",
    "    input = input_ori[0].numpy().transpose(1, 2, 0)\n",
    "    input_gray = cv.cvtColor(input, cv.COLOR_BGR2GRAY)\n",
    "    input_gray = (input_gray - input_gray.min()) / (input_gray.max() - input_gray.min())\n",
    "\n",
    "    gen_image = gen_image[0].numpy().transpose(1, 2, 0)\n",
    "    gen_image_gray = cv.cvtColor(gen_image, cv.COLOR_BGR2GRAY)\n",
    "    gen_image_gray = (gen_image_gray - gen_image_gray.min()) / (gen_image_gray.max() - gen_image_gray.min())\n",
    "\n",
    "    target = (((target[0] - target[0].min()) * 255) / (target[0].max() - target[0].min())).numpy().transpose(1, 2, 0)\n",
    "\n",
    "    error_map = np.absolute(input_gray - gen_image_gray)\n",
    "   \n",
    "    error_map = (error_map - np.min(error_map)) / (np.max(error_map) - np.min(error_map))\n",
    "    error_map = (error_map * 255).astype(np.uint8)\n",
    "\n",
    "    input_gray = (input_gray * 255).astype(np.uint8)\n",
    "    gen_image_gray = (gen_image_gray * 255).astype(np.uint8)\n",
    "\n",
    "    if save:\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        cv.imwrite(save_dir + '{:s}'.format(name) + '.png', error_map)\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "test_data = DatasetFromFolder(data_dir, subfolder='test', direction= 'AtoB', transform=test_transform)\n",
    "test_data_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                               batch_size=1,\n",
    "                                               shuffle=False)\n",
    "\n",
    "G = Generator(3, ngf, 3)\n",
    "D = SwinTDiscriminator()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    D = nn.DataParallel(D)\n",
    "    G = nn.DataParallel(G)\n",
    "D.cuda()\n",
    "G.cuda()\n",
    "\n",
    "\n",
    "D.load_state_dict(torch.load('/kaggle/working/saved-model/100discriminator_param.pkl'))\n",
    "\n",
    "\n",
    "for i, (input, target, img_target2) in enumerate(test_data_loader):\n",
    "    \n",
    "    \n",
    "    input_name = test_data.image_filenames[i] \n",
    "\n",
    "    input_np = (((input[0] - input[0].min()) * 255) / (input[0].max() - input[0].min())).numpy().transpose(1, 2, 0).astype(np.uint8)\n",
    "\n",
    "    input_ori = transforms.ToPILImage()(input_np)\n",
    "    input_ori = test_transform(input_ori)\n",
    "\n",
    "    input_ori = torch.unsqueeze(input_ori, 0)        \n",
    "\n",
    "    x_ = Variable(input_ori.cuda())\n",
    "    y_ = Variable(target.cuda())\n",
    "\n",
    "    gen_image = G(x_)\n",
    "    gen_image = gen_image.cpu().data\n",
    "\n",
    "    save_error_maps_gray(input_name, input_ori, target, gen_image,\n",
    "                              i, save=True, save_dir=save_error_dir)\n",
    "    print('%d images are generated.' % (i + 1))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
